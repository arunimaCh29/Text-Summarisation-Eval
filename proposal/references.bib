@article{go2009twitter,
  author = {Go, Alec and Bhayani, Richa and Huang, Lei},
  title = {Twitter Sentiment Classification Using Distant Supervision},
  journal = {CS224N Project Report, Stanford},
  volume = {1},
  year = {2009},
  pages = {12},
  url = {http://cs.stanford.edu/people/alecmgo/trainingandtestdata.zip},
  note = {Accessed: 2025-03-27}
}

@article{raffel2020exploring,
  title={Exploring the limits of transfer learning with a unified text-to-text transformer},
  author={Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J},
  journal={Journal of machine learning research},
  volume={21},
  number={140},
  pages={1--67},
  year={2020}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@article{barbella2022rouge,
  title={Rouge metric evaluation for text summarization techniques},
  author={Barbella, Marcello and Tortora, Genoveffa},
  journal={Available at SSRN 4120317},
  year={2022}
}

@misc{perspectiveapi,
  author       = {{Jigsaw and Google}},
  title        = {Perspective API},
  howpublished = {\url{https://www.perspectiveapi.com/}},
  year         = {2017},
  note         = {Accessed: 2025-05-03}
}

@misc{Detoxify,
  title={Detoxify},
  author={Hanu, Laura and {Unitary team}},
  howpublished={Github. https://github.com/unitaryai/detoxify},
  year={2020}
}


@misc{alex2019multinews,
    title={Multi-News: a Large-Scale Multi-Document Summarization Dataset and Abstractive Hierarchical Model},
    author={Alexander R. Fabbri and Irene Li and Tianwei She and Suyi Li and Dragomir R. Radev},
    year={2019},
    eprint={1906.01749},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
@article{thota2024attacks,
  title={Attacks against Abstractive Text Summarization Models through Lead Bias and Influence Functions},
  author={Thota, Poojitha and Nilizadeh, Shirin},
  journal={arXiv preprint arXiv:2410.20019},
  year={2024}
}

@article{gehman2020realtoxicityprompts,
  title={RealToxicityPrompts: Evaluating neural toxic degeneration in language models},
  author={Gehman, Samuel and Gururangan, Suchin and Sap, Maarten and Choi, Yejin and Smith, Noah A},
  journal={Findings of EMNLP},
  year={2020}
}

@article{liu2021experts,
  title={DExperts: Decoding-Time Controlled Text Generation with Experts and Anti-Experts},
  author={Liu, Eric and Khandelwal, Urvashi and Ju, Da and Krishnan, Satyen and McMillan-Major, Angela and Neubig, Graham},
  journal={arXiv preprint arXiv:2105.03079},
  year={2021}
}

@inproceedings{kaur2022mitigating,
  title={Mitigating Social Bias in Text Summarization Models},
  author={Kaur, Trisha and Krishna, Sarthak and Shah, Rohit and Sangwan, Anirudh and Subramaniam, Laxmi and Sharma, Aparna},
  booktitle={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2022}
}

@inproceedings{krishna2021halueval,
  title={HaluEval: Benchmarking the Faithfulness of Language Models in Multi-Document Summarization},
  author={Krishna, Kalpesh and Zhang, Zhijing and Iyyer, Mohit},
  booktitle={EMNLP},
  year={2021}
}

@article{narayan2018multinews,
  title={Don't Give Me the Details, Just the Summary! Topic-Aware Convolutional Neural Networks for Extreme Summarization},
  author={Narayan, Shashi and Cohen, Shay B and Lapata, Mirella},
  journal={EMNLP},
  year={2018}
}

@article{zhang2020pegasus,
  title={PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization},
  author={Zhang, Jingqing and Zhao, Yao and Saleh, Mohammad and Liu, Peter J},
  journal={ICML},
  year={2020}
}

@inproceedings{fabbri2021sumeval,
  title={SummEval: Re-evaluating Summarization Evaluation},
  author={Fabbri, Alexander R and Kryściński, Wojciech and McCann, Bryan and Xiong, Caiming and Socher, Richard and Radev, Dragomir},
  booktitle={NAACL},
  year={2021}
}


@inproceedings{volske-etal-2017-tl,
    title = "{TL};{DR}: Mining {R}eddit to Learn Automatic Summarization",
    author = {V{"o}lske, Michael  and
      Potthast, Martin  and
      Syed, Shahbaz  and
      Stein, Benno},
    booktitle = "Proceedings of the Workshop on New Frontiers in Summarization",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-4508",
    doi = "10.18653/v1/W17-4508",
    pages = "59--63",
    abstract = "Recent advances in automatic text summarization have used deep neural networks to generate high-quality abstractive summaries, but the performance of these models strongly depends on large amounts of suitable training data. We propose a new method for mining social media for author-provided summaries, taking advantage of the common practice of appending a {``}TL;DR{''} to long posts. A case study using a large Reddit crawl yields the Webis-TLDR-17 dataset, complementing existing corpora primarily from the news genre. Our technique is likely applicable to other social media sites and general web crawls.",
}
